{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STEP 2Intent Recognition System\n",
        "\n",
        "This notebook implements an intent recognition system for restaurant-related queries with the following main intents:\n",
        "\n",
        "- LOCATION_QUERY: Location-based restaurant queries (e.g., \"Ankara'da pizza yeri\")\n",
        "- CUISINE_QUERY: Cuisine/food type queries (e.g., \"İyi burger nerede?\")\n",
        "- FEATURE_QUERY: Restaurant feature/attribute queries (e.g., \"Temiz ve hızlı servis\")\n",
        "- RATING_QUERY: Rating/quality-based queries (e.g., \"En iyi restoran\")\n",
        "- COMPARISON_QUERY: Restaurant comparison queries (e.g., \"X ile Y'yi karşılaştır\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "200e28a403a14d28ba1e9dc5b860e4e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-18 14:38:24 INFO: Downloaded file to /Users/Serra/stanza_resources/resources.json\n",
            "2025-08-18 14:38:24 INFO: Downloading default packages for language: tr (Turkish) ...\n",
            "2025-08-18 14:38:24 INFO: File exists: /Users/Serra/stanza_resources/tr/default.zip\n",
            "2025-08-18 14:38:26 INFO: Finished downloading models and saved to /Users/Serra/stanza_resources\n",
            "2025-08-18 14:38:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "070b92cabe714a3f808c6a7dac0f4824",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-18 14:38:27 INFO: Downloaded file to /Users/Serra/stanza_resources/resources.json\n",
            "2025-08-18 14:38:27 WARNING: Language tr package default expects mwt, which has been added\n",
            "2025-08-18 14:38:27 INFO: Loading these models for language: tr (Turkish):\n",
            "=============================\n",
            "| Processor | Package       |\n",
            "-----------------------------\n",
            "| tokenize  | imst          |\n",
            "| mwt       | imst          |\n",
            "| pos       | imst_charlm   |\n",
            "| lemma     | imst_nocharlm |\n",
            "=============================\n",
            "\n",
            "2025-08-18 14:38:27 INFO: Using device: cpu\n",
            "2025-08-18 14:38:27 INFO: Loading: tokenize\n",
            "2025-08-18 14:38:27 INFO: Loading: mwt\n",
            "2025-08-18 14:38:27 INFO: Loading: pos\n",
            "2025-08-18 14:38:28 INFO: Loading: lemma\n",
            "2025-08-18 14:38:28 INFO: Done loading processors!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import stanza\n",
        "import re\n",
        "\n",
        "# Download Turkish model if not already present\n",
        "stanza.download(\"tr\")\n",
        "nlp_stanza = stanza.Pipeline(\"tr\", processors=\"tokenize,pos,lemma\", use_gpu=torch.cuda.is_available())\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create Training Dataset\n",
        "\n",
        "First, let's create a synthetic dataset for training our intent recognition model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (50, 2)\n",
            "\n",
            "Sample queries per intent:\n",
            "                         query            intent\n",
            "0         Ankara'da pizza yeri    LOCATION_QUERY\n",
            "1   Çankaya'da iyi restoranlar    LOCATION_QUERY\n",
            "10          İyi burger nerede?     CUISINE_QUERY\n",
            "11           En güzel lahmacun     CUISINE_QUERY\n",
            "20       Temiz ve hızlı servis     FEATURE_QUERY\n",
            "21        Çocuk dostu restoran     FEATURE_QUERY\n",
            "30             En iyi restoran      RATING_QUERY\n",
            "31     En çok puan alan yerler      RATING_QUERY\n",
            "40      X ile Y'yi karşılaştır  COMPARISON_QUERY\n",
            "41      Hangi kebapçı daha iyi  COMPARISON_QUERY\n"
          ]
        }
      ],
      "source": [
        "# Sample queries for each intent type\n",
        "training_data = {\n",
        "    'LOCATION_QUERY': [\n",
        "        \"Ankara'da pizza yeri\",\n",
        "        \"Çankaya'da iyi restoranlar\",\n",
        "        \"Kızılay bölgesinde ne var\",\n",
        "        \"Keçiören'de kahvaltı mekanları\",\n",
        "        \"Tunalı'da akşam yemeği\",\n",
        "        \"Ulus'ta kebapçı\",\n",
        "        \"Bahçelievler'de cafe\",\n",
        "        \"Batıkent'te dönerci\",\n",
        "        \"Mamak'ta lokanta\",\n",
        "        \"Sincan'da restaurant önerisi\"\n",
        "    ],\n",
        "    'CUISINE_QUERY': [\n",
        "        \"İyi burger nerede?\",\n",
        "        \"En güzel lahmacun\",\n",
        "        \"Mantı yapan yerler\",\n",
        "        \"Nerede güzel pizza var\",\n",
        "        \"İskender kebap nerede yenir\",\n",
        "        \"Ev yemekleri yapan lokanta\",\n",
        "        \"Çiğ köfte dürüm\",\n",
        "        \"Pide salonu önerisi\",\n",
        "        \"Balık restoranı\",\n",
        "        \"Vejeteryan restoran\"\n",
        "    ],\n",
        "    'FEATURE_QUERY': [\n",
        "        \"Temiz ve hızlı servis\",\n",
        "        \"Çocuk dostu restoran\",\n",
        "        \"Manzaralı mekan\",\n",
        "        \"Bahçeli cafe\",\n",
        "        \"Canlı müzik olan yerler\",\n",
        "        \"Büyük grup için uygun\",\n",
        "        \"Sessiz çalışma mekanı\",\n",
        "        \"Otoparklı restaurant\",\n",
        "        \"WiFi olan kafeler\",\n",
        "        \"Alkollü mekan\"\n",
        "    ],\n",
        "    'RATING_QUERY': [\n",
        "        \"En iyi restoran\",\n",
        "        \"En çok puan alan yerler\",\n",
        "        \"Popüler mekanlar\",\n",
        "        \"Yüksek puanlı lokantalar\",\n",
        "        \"Müşteri memnuniyeti yüksek\",\n",
        "        \"Tavsiye edilen restoranlar\",\n",
        "        \"Yıldızı yüksek olan yerler\",\n",
        "        \"En beğenilen kafeler\",\n",
        "        \"Kaliteli restoranlar\",\n",
        "        \"İyi yorumlar alan mekanlar\"\n",
        "    ],\n",
        "    'COMPARISON_QUERY': [\n",
        "        \"X ile Y'yi karşılaştır\",\n",
        "        \"Hangi kebapçı daha iyi\",\n",
        "        \"Pizza House mı Dominos mu\",\n",
        "        \"En iyi burger hangisi\",\n",
        "        \"Hangi cafe daha uygun\",\n",
        "        \"X restoranı mı Y lokantası mı\",\n",
        "        \"Fiyat karşılaştırması\",\n",
        "        \"Hangisinin servisi daha iyi\",\n",
        "        \"Kalite fiyat karşılaştırması\",\n",
        "        \"Menü çeşitliliği karşılaştırma\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "queries = []\n",
        "intents = []\n",
        "for intent, query_list in training_data.items():\n",
        "    queries.extend(query_list)\n",
        "    intents.extend([intent] * len(query_list))\n",
        "\n",
        "df_train = pd.DataFrame({\n",
        "    'query': queries,\n",
        "    'intent': intents\n",
        "})\n",
        "\n",
        "print(\"Training data shape:\", df_train.shape)\n",
        "print(\"\\nSample queries per intent:\")\n",
        "print(df_train.groupby('intent').head(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intent encoding mapping:\n",
            "COMPARISON_QUERY: 0\n",
            "CUISINE_QUERY: 1\n",
            "FEATURE_QUERY: 2\n",
            "LOCATION_QUERY: 3\n",
            "RATING_QUERY: 4\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove punctuation except apostrophes in Turkish words\n",
        "    text = re.sub(r'[^\\w\\s\\'\\u0300-\\u036f\\u0130\\u0131]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_train['processed_query'] = df_train['query'].apply(preprocess_text)\n",
        "\n",
        "# Create label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_train['intent_encoded'] = label_encoder.fit_transform(df_train['intent'])\n",
        "\n",
        "print(\"Intent encoding mapping:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{label}: {i}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (50, 384)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the sentence transformer model\n",
        "model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
        "encoder = SentenceTransformer(model_name)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = encoder.encode(df_train['processed_query'].tolist())\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create PyTorch Dataset and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntentDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = torch.FloatTensor(embeddings)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "class IntentClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super(IntentClassifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.layer3 = nn.Linear(hidden_dim // 2, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/50], Loss: 0.9910\n",
            "Epoch [20/50], Loss: 0.2673\n",
            "Epoch [30/50], Loss: 0.0622\n",
            "Epoch [40/50], Loss: 0.0446\n",
            "Epoch [50/50], Loss: 0.0845\n"
          ]
        }
      ],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    embeddings, \n",
        "    df_train['intent_encoded'].values,\n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=df_train['intent_encoded'].values\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IntentDataset(X_train, y_train)\n",
        "test_dataset = IntentDataset(X_test, y_test)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = embeddings.shape[1]  # 384 for the chosen model\n",
        "hidden_dim = 256\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = IntentClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_embeddings, batch_labels in train_loader:\n",
        "        batch_embeddings = batch_embeddings.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_embeddings)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "COMPARISON_QUERY       1.00      0.50      0.67         2\n",
            "   CUISINE_QUERY       0.25      0.50      0.33         2\n",
            "   FEATURE_QUERY       0.50      0.50      0.50         2\n",
            "  LOCATION_QUERY       1.00      0.50      0.67         2\n",
            "    RATING_QUERY       1.00      1.00      1.00         2\n",
            "\n",
            "        accuracy                           0.60        10\n",
            "       macro avg       0.75      0.60      0.63        10\n",
            "    weighted avg       0.75      0.60      0.63        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_embeddings, batch_labels in test_loader:\n",
        "        batch_embeddings = batch_embeddings.to(device)\n",
        "        outputs = model(batch_embeddings)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.numpy())\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Intent Recognition Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing intent recognition:\n",
            "\n",
            "Query: Ankara'da en iyi pizza\n",
            "Predicted Intent: LOCATION_QUERY\n",
            "Confidence: 0.8978\n",
            "\n",
            "Query: Hangi restoran daha iyi?\n",
            "Predicted Intent: COMPARISON_QUERY\n",
            "Confidence: 0.8459\n",
            "\n",
            "Query: Temiz ve ferah mekan\n",
            "Predicted Intent: FEATURE_QUERY\n",
            "Confidence: 0.9734\n",
            "\n",
            "Query: En yüksek puanlı yerler\n",
            "Predicted Intent: RATING_QUERY\n",
            "Confidence: 0.9997\n",
            "\n",
            "Query: Çankaya'da kahvaltı\n",
            "Predicted Intent: LOCATION_QUERY\n",
            "Confidence: 0.9985\n"
          ]
        }
      ],
      "source": [
        "def predict_intent(query, encoder=encoder, model=model, label_encoder=label_encoder):\n",
        "    # Preprocess query\n",
        "    processed_query = preprocess_text(query)\n",
        "    \n",
        "    # Generate embedding\n",
        "    with torch.no_grad():\n",
        "        embedding = encoder.encode([processed_query])\n",
        "        embedding_tensor = torch.FloatTensor(embedding).to(device)\n",
        "        \n",
        "        # Get model prediction\n",
        "        model.eval()\n",
        "        output = model(embedding_tensor)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        \n",
        "        # Get predicted intent\n",
        "        predicted_intent = label_encoder.inverse_transform(predicted.cpu().numpy())[0]\n",
        "        \n",
        "        # Get confidence scores\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        confidence = probabilities.max().item()\n",
        "        \n",
        "        return {\n",
        "            'intent': predicted_intent,\n",
        "            'confidence': confidence\n",
        "        }\n",
        "\n",
        "# Test the function with some example queries\n",
        "test_queries = [\n",
        "    \"Ankara'da en iyi pizza\",\n",
        "    \"Hangi restoran daha iyi?\",\n",
        "    \"Temiz ve ferah mekan\",\n",
        "    \"En yüksek puanlı yerler\",\n",
        "    \"Çankaya'da kahvaltı\"\n",
        "]\n",
        "\n",
        "print(\"Testing intent recognition:\")\n",
        "for query in test_queries:\n",
        "    result = predict_intent(query)\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"Predicted Intent: {result['intent']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save the Model and Required Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and components saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'intent_classifier_model.pth')\n",
        "\n",
        "# Save the label encoder\n",
        "with open('intent_label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"Model and components saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FASE 2: Çekirdek NLP Modülleri \n",
        "## 2.1 Metin Ön İşleme Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List, Dict, Set\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        # Turkish stop words\n",
        "        self.turkish_stopwords = {\n",
        "            'bir', 'bu', 'da', 'de', 'den', 'dır', 'dir', 'dır', 'için', 'ile', \n",
        "            'ise', 've', 'var', 'yok', 'olan', 'olur', 'şu', 'o', 'ki', 'mi', \n",
        "            'mı', 'mu', 'mü', 'ne', 'ama', 'fakat', 'veya', 'ya', 'gibi', 'kadar',\n",
        "            'sonra', 'önce', 'çok', 'az', 'daha', 'en', 'çünkü', 'hem', 'ya da',\n",
        "            'ancak', 'lakin', 'sadece', 'yalnız', 'hatta', 'bile', 'bütün', 'tüm'\n",
        "        }\n",
        "        \n",
        "        # Turkish character mapping for normalization\n",
        "        self.char_map = {\n",
        "            'ç': 'c', 'ğ': 'g', 'ı': 'i', 'ö': 'o', 'ş': 's', 'ü': 'u',\n",
        "            'Ç': 'C', 'Ğ': 'G', 'İ': 'I', 'Ö': 'O', 'Ş': 'S', 'Ü': 'U'\n",
        "        }\n",
        "    \n",
        "    def normalize_turkish_chars(self, text: str, keep_turkish: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Normalize Turkish characters\n",
        "        keep_turkish: If True, keeps Turkish chars, if False converts to ASCII\n",
        "        \"\"\"\n",
        "        if not keep_turkish:\n",
        "            for turkish_char, ascii_char in self.char_map.items():\n",
        "                text = text.replace(turkish_char, ascii_char)\n",
        "        return text\n",
        "    \n",
        "    def remove_punctuation(self, text: str) -> str:\n",
        "        \"\"\"Remove punctuation while preserving apostrophes in Turkish words\"\"\"\n",
        "        # Keep apostrophes and Turkish letters\n",
        "        text = re.sub(r'[^\\w\\s\\'\\u00C0-\\u017F\\u0130\\u0131]', ' ', text)\n",
        "        return text\n",
        "    \n",
        "    def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
        "        \"\"\"Remove Turkish stop words\"\"\"\n",
        "        return [token for token in tokens if token.lower() not in self.turkish_stopwords]\n",
        "    \n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple tokenization\"\"\"\n",
        "        return text.split()\n",
        "    \n",
        "    def lemmatize_with_stanza(self, text: str) -> List[str]:\n",
        "        \"\"\"Lemmatize using Stanza (already initialized in your notebook)\"\"\"\n",
        "        doc = nlp_stanza(text)\n",
        "        lemmas = []\n",
        "        for sentence in doc.sentences:\n",
        "            for word in sentence.words:\n",
        "                lemmas.append(word.lemma)\n",
        "        return lemmas\n",
        "    \n",
        "    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:\n",
        "        \"\"\"Extract n-grams from tokens\"\"\"\n",
        "        if len(tokens) < n:\n",
        "            return []\n",
        "        return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "    \n",
        "    def process_text(self, text: str, \n",
        "                    normalize_chars: bool = True,\n",
        "                    remove_stops: bool = True,\n",
        "                    use_lemmatization: bool = True,\n",
        "                    extract_bigrams: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Complete text processing pipeline\n",
        "        Returns processed tokens, lemmas, and n-grams\n",
        "        \"\"\"\n",
        "        # Step 1: Basic cleaning\n",
        "        processed_text = text.lower().strip()\n",
        "        \n",
        "        # Step 2: Character normalization\n",
        "        if normalize_chars:\n",
        "            processed_text = self.normalize_turkish_chars(processed_text, keep_turkish=True)\n",
        "        \n",
        "        # Step 3: Remove punctuation\n",
        "        processed_text = self.remove_punctuation(processed_text)\n",
        "        \n",
        "        # Step 4: Normalize whitespace\n",
        "        processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "        \n",
        "        # Step 5: Tokenization\n",
        "        tokens = self.tokenize(processed_text)\n",
        "        \n",
        "        # Step 6: Stop word removal\n",
        "        if remove_stops:\n",
        "            tokens = self.remove_stopwords(tokens)\n",
        "        \n",
        "        # Step 7: Lemmatization\n",
        "        lemmas = []\n",
        "        if use_lemmatization and tokens:\n",
        "            lemmas = self.lemmatize_with_stanza(' '.join(tokens))\n",
        "        \n",
        "        # Step 8: N-gram extraction\n",
        "        bigrams = []\n",
        "        trigrams = []\n",
        "        if extract_bigrams and len(tokens) >= 2:\n",
        "            bigrams = self.extract_ngrams(tokens, 2)\n",
        "            if len(tokens) >= 3:\n",
        "                trigrams = self.extract_ngrams(tokens, 3)\n",
        "        \n",
        "        return {\n",
        "            'original_text': text,\n",
        "            'processed_text': processed_text,\n",
        "            'tokens': tokens,\n",
        "            'lemmas': lemmas,\n",
        "            'bigrams': bigrams,\n",
        "            'trigrams': trigrams,\n",
        "            'token_count': len(tokens)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "bakabiliriz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from typing import List, Dict, Tuple, Set\n",
        "\n",
        "class RestaurantNER:\n",
        "    def __init__(self):\n",
        "        # Location entities (Turkish cities, districts)\n",
        "        self.location_keywords = {\n",
        "            # Major cities\n",
        "            'ankara', 'istanbul', 'izmir', 'bursa', 'antalya', 'adana', 'konya', 'gaziantep', 'kayseri', 'mersin',\n",
        "            # Ankara districts\n",
        "            'çankaya', 'keçiören', 'yenimahalle', 'mamak', 'sincan', 'altındağ', 'etimesgut', 'gölbaşı', 'pursaklar',\n",
        "            'kızılay', 'tunalı', 'bahçelievler', 'batıkent', 'ulus', 'beşevler', 'dikmen', 'çayyolu', 'oran',\n",
        "            # Common location words\n",
        "            'merkez', 'şehir', 'bölge', 'mahalle', 'cadde', 'sokak', 'plaza', 'avm', 'mall'\n",
        "        }\n",
        "        \n",
        "        # Cuisine types\n",
        "        self.cuisine_keywords = {\n",
        "            # Turkish cuisine\n",
        "            'kebap', 'kebab', 'döner', 'lahmacun', 'pide', 'mantı', 'çorbası', 'köfte', 'iskender', 'adana',\n",
        "            'urfa', 'beyti', 'şiş', 'tavuk', 'et', 'balık', 'deniz', 'ürünleri',\n",
        "            # International cuisine\n",
        "            'pizza', 'burger', 'hamburger', 'makarna', 'spagetti', 'çin', 'japon', 'sushi', 'meksika',\n",
        "            'hint', 'arap', 'lübnan', 'fransız', 'i̇talyan', 'amerikan',\n",
        "            # Food types\n",
        "            'kahvaltı', 'öğlen', 'akşam', 'yemek', 'yemeği', 'atıştırmalık', 'tatlı', 'içecek', 'kahve', 'çay',\n",
        "            'vejeteryan', 'vegan', 'organik', 'sağlıklı', 'diyet'\n",
        "        }\n",
        "        \n",
        "        # Price indicators\n",
        "        self.price_keywords = {\n",
        "            'ucuz', 'pahalı', 'uygun', 'fiyat', 'fiyatlı', 'ekonomik', 'bütçe', 'para', 'lira', 'tl',\n",
        "            'student', 'öğrenci', 'indirim', 'kampanya', 'promosyon', 'hesaplı', 'makul', 'pahallı'\n",
        "        }\n",
        "        \n",
        "        # Rating/Quality indicators\n",
        "        self.rating_keywords = {\n",
        "            'iyi', 'güzel', 'harika', 'mükemmel', 'süper', 'muhteşem', 'enfes', 'lezzetli', 'nefis',\n",
        "            'kötü', 'berbat', 'leş', 'iğrenç', 'beğenmedim', 'beğenmem',\n",
        "            'temiz', 'hijyen', 'hijyenik', 'pis', 'kirli',\n",
        "            'hızlı', 'yavaş', 'geç', 'erken', 'servis', 'garson', 'hizmet',\n",
        "            'puanlı', 'puan', 'yıldız', 'yıldızlı', 'değerlendirme', 'yorum', 'tavsiye',\n",
        "            'popüler', 'ünlü', 'meşhur', 'tanınan', 'bilinir'\n",
        "        }\n",
        "        \n",
        "        # Additional features\n",
        "        self.feature_keywords = {\n",
        "            'bahçe', 'bahçeli', 'teras', 'açık', 'kapalı', 'klimalı', 'ısıtmalı',\n",
        "            'müzik', 'canlı', 'sessiz', 'sakin', 'kalabalık', 'gürültülü',\n",
        "            'wifi', 'internet', 'otopark', 'park', 'valet',\n",
        "            'çocuk', 'dostu', 'aile', 'sevgili', 'grup', 'büyük',\n",
        "            'manzara', 'manzaralı', 'görünüm', 'şehir', 'deniz', 'dağ'\n",
        "        }\n",
        "    \n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Extract restaurant-related entities from text\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        tokens = text_lower.split()\n",
        "        \n",
        "        entities = {\n",
        "            'LOCATION': [],\n",
        "            'CUISINE': [],\n",
        "            'PRICE': [],\n",
        "            'RATING': [],\n",
        "            'FEATURE': []\n",
        "        }\n",
        "        \n",
        "        # Extract location entities\n",
        "        for token in tokens:\n",
        "            if any(loc in token for loc in self.location_keywords):\n",
        "                entities['LOCATION'].append(token)\n",
        "        \n",
        "        # Extract cuisine entities\n",
        "        for token in tokens:\n",
        "            if any(cuisine in token for cuisine in self.cuisine_keywords):\n",
        "                entities['CUISINE'].append(token)\n",
        "        \n",
        "        # Extract price entities\n",
        "        for token in tokens:\n",
        "            if any(price in token for price in self.price_keywords):\n",
        "                entities['PRICE'].append(token)\n",
        "        \n",
        "        # Extract rating entities\n",
        "        for token in tokens:\n",
        "            if any(rating in token for rating in self.rating_keywords):\n",
        "                entities['RATING'].append(token)\n",
        "        \n",
        "        # Extract feature entities\n",
        "        for token in tokens:\n",
        "            if any(feature in token for feature in self.feature_keywords):\n",
        "                entities['FEATURE'].append(token)\n",
        "        \n",
        "        # Remove duplicates\n",
        "        for entity_type in entities:\n",
        "            entities[entity_type] = list(set(entities[entity_type]))\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    def get_entity_context(self, text: str, entity: str, window: int = 3) -> str:\n",
        "        \"\"\"\n",
        "        Get context around an entity for better understanding\n",
        "        \"\"\"\n",
        "        tokens = text.lower().split()\n",
        "        entity_positions = [i for i, token in enumerate(tokens) if entity in token]\n",
        "        \n",
        "        contexts = []\n",
        "        for pos in entity_positions:\n",
        "            start = max(0, pos - window)\n",
        "            end = min(len(tokens), pos + window + 1)\n",
        "            context = ' '.join(tokens[start:end])\n",
        "            contexts.append(context)\n",
        "        \n",
        "        return ' | '.join(contexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.3 Embedding & Similarity Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class SimilarityEngine:\n",
        "    def __init__(self, model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "        self.encoder = SentenceTransformer(model_name)\n",
        "        self.restaurant_embeddings = None\n",
        "        self.restaurant_data = None\n",
        "        self.text_processor = TextProcessor()\n",
        "        self.ner_processor = RestaurantNER()\n",
        "    \n",
        "    def create_restaurant_data(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create sample restaurant data for demonstration\n",
        "        In real implementation, this would come from your database\n",
        "        \"\"\"\n",
        "        restaurants = [\n",
        "            {\n",
        "                'id': 1, 'name': 'Köfteci Ramiz', 'location': 'Kızılay', \n",
        "                'cuisine': 'Türk Mutfağı', 'rating': 4.5, 'price_range': 'Orta',\n",
        "                'features': 'Hızlı servis, Temiz, Aile dostu',\n",
        "                'description': 'Kızılayda ünlü köfteci, lezzetli köfte ve lahmacun'\n",
        "            },\n",
        "            {\n",
        "                'id': 2, 'name': 'Pizza Palace', 'location': 'Çankaya', \n",
        "                'cuisine': 'İtalyan', 'rating': 4.2, 'price_range': 'Pahalı',\n",
        "                'features': 'Canlı müzik, Manzaralı, Otopark',\n",
        "                'description': 'Çankayada en iyi pizza, İtalyan şefi, harika manzara'\n",
        "            },\n",
        "            {\n",
        "                'id': 3, 'name': 'Burger King', 'location': 'Tunalı', \n",
        "                'cuisine': 'Fast Food', 'rating': 3.8, 'price_range': 'Ucuz',\n",
        "                'features': 'Hızlı servis, Çocuk dostu, WiFi',\n",
        "                'description': 'Tunalıda burger, hızlı servis, öğrenci dostu fiyatlar'\n",
        "            },\n",
        "            {\n",
        "                'id': 4, 'name': 'Balık Restaurant', 'location': 'Bahçelievler', \n",
        "                'cuisine': 'Deniz Ürünleri', 'rating': 4.7, 'price_range': 'Pahalı',\n",
        "                'features': 'Taze balık, Manzaralı, Sessiz',\n",
        "                'description': 'Taze deniz ürünleri, kaliteli servis, romantik atmosfer'\n",
        "            },\n",
        "            {\n",
        "                'id': 5, 'name': 'Mantı Evi', 'location': 'Keçiören', \n",
        "                'cuisine': 'Türk Mutfağı', 'rating': 4.1, 'price_range': 'Ucuz',\n",
        "                'features': 'Ev yemeği, Samimi, Ekonomik',\n",
        "                'description': 'Ev yapımı mantı, sıcak atmosfer, uygun fiyatlar'\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        return pd.DataFrame(restaurants)\n",
        "    \n",
        "    def create_restaurant_embeddings(self, restaurant_data: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Create embeddings for restaurant descriptions and features\n",
        "        \"\"\"\n",
        "        self.restaurant_data = restaurant_data\n",
        "        \n",
        "        # Combine name, cuisine, features, and description for embedding\n",
        "        restaurant_texts = []\n",
        "        for _, row in restaurant_data.iterrows():\n",
        "            combined_text = f\"{row['name']} {row['location']} {row['cuisine']} {row['features']} {row['description']}\"\n",
        "            restaurant_texts.append(combined_text)\n",
        "        \n",
        "        self.restaurant_embeddings = self.encoder.encode(restaurant_texts)\n",
        "        print(f\"Created embeddings for {len(restaurant_texts)} restaurants\")\n",
        "    \n",
        "    def encode_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Encode a user query into embeddings\n",
        "        \"\"\"\n",
        "        # Process the query\n",
        "        processed = self.text_processor.process_text(query)\n",
        "        \n",
        "        # Extract entities\n",
        "        entities = self.ner_processor.extract_entities(query)\n",
        "        \n",
        "        # Enhance query with extracted entities\n",
        "        enhanced_query = query\n",
        "        for entity_type, entity_list in entities.items():\n",
        "            if entity_list:\n",
        "                enhanced_query += f\" {' '.join(entity_list)}\"\n",
        "        \n",
        "        # Create embedding\n",
        "        query_embedding = self.encoder.encode([enhanced_query])\n",
        "        return query_embedding[0], entities\n",
        "    \n",
        "    def calculate_similarity(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Calculate cosine similarity between query and restaurant embeddings\n",
        "        \"\"\"\n",
        "        if self.restaurant_embeddings is None:\n",
        "            raise ValueError(\"Restaurant embeddings not created. Call create_restaurant_embeddings first.\")\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        similarities = cosine_similarity([query_embedding], self.restaurant_embeddings)[0]\n",
        "        \n",
        "        # Get top-k results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        results = [(idx, similarities[idx]) for idx in top_indices]\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def semantic_search(self, query: str, top_k: int = 5, min_similarity: float = 0.3) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform semantic search on restaurants\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_embedding, entities = self.encode_query(query)\n",
        "        \n",
        "        # Calculate similarities\n",
        "        similarities = self.calculate_similarity(query_embedding, top_k)\n",
        "        \n",
        "        # Filter by minimum similarity\n",
        "        filtered_results = [(idx, score) for idx, score in similarities if score >= min_similarity]\n",
        "        \n",
        "        # Prepare results\n",
        "        results = []\n",
        "        for idx, score in filtered_results:\n",
        "            restaurant = self.restaurant_data.iloc[idx].to_dict()\n",
        "            restaurant['similarity_score'] = score\n",
        "            results.append(restaurant)\n",
        "        \n",
        "        return {\n",
        "            'query': query,\n",
        "            'extracted_entities': entities,\n",
        "            'results': results,\n",
        "            'total_results': len(results)\n",
        "        }\n",
        "    \n",
        "    def find_similar_restaurants(self, restaurant_id: int, top_k: int = 3) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Find restaurants similar to a given restaurant\n",
        "        \"\"\"\n",
        "        if self.restaurant_embeddings is None:\n",
        "            raise ValueError(\"Restaurant embeddings not created.\")\n",
        "        \n",
        "        # Get the restaurant embedding\n",
        "        restaurant_embedding = self.restaurant_embeddings[restaurant_id]\n",
        "        \n",
        "        # Calculate similarities (exclude the restaurant itself)\n",
        "        similarities = cosine_similarity([restaurant_embedding], self.restaurant_embeddings)[0]\n",
        "        similarities[restaurant_id] = -1  # Exclude self\n",
        "        \n",
        "        # Get top-k similar restaurants\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:  # Only include positive similarities\n",
        "                restaurant = self.restaurant_data.iloc[idx].to_dict()\n",
        "                restaurant['similarity_score'] = similarities[idx]\n",
        "                results.append(restaurant)\n",
        "        \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created embeddings for 5 restaurants\n",
            "Analyzing query: 'Kızılay'da iyi köfte nerede yenir?'\n",
            "==================================================\n",
            "Intent: CUISINE_QUERY (confidence: 0.9123)\n",
            "Processed tokens: [\"kızılay'da\", 'iyi', 'köfte', 'nerede', 'yenir']\n",
            "Bigrams: [\"kızılay'da iyi\", 'iyi köfte', 'köfte nerede', 'nerede yenir']\n",
            "Extracted entities: {'LOCATION': [\"kızılay'da\"], 'CUISINE': ['köfte'], 'PRICE': [], 'RATING': ['iyi'], 'FEATURE': []}\n",
            "Top 3 matching restaurants:\n",
            "  1. Köfteci Ramiz (Kızılay) - Score: 0.6273\n",
            "  2. Mantı Evi (Keçiören) - Score: 0.5093\n",
            "  3. Balık Restaurant (Bahçelievler) - Score: 0.4356\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analyzing query: 'Çankaya'da pahalı olmayan pizza yeri'\n",
            "==================================================\n",
            "Intent: LOCATION_QUERY (confidence: 0.7552)\n",
            "Processed tokens: [\"çankaya'da\", 'pahalı', 'olmayan', 'pizza', 'yeri']\n",
            "Bigrams: [\"çankaya'da pahalı\", 'pahalı olmayan', 'olmayan pizza', 'pizza yeri']\n",
            "Extracted entities: {'LOCATION': [\"çankaya'da\"], 'CUISINE': ['pizza'], 'PRICE': ['pahalı'], 'RATING': [], 'FEATURE': []}\n",
            "Top 3 matching restaurants:\n",
            "  1. Pizza Palace (Çankaya) - Score: 0.7543\n",
            "  2. Mantı Evi (Keçiören) - Score: 0.5183\n",
            "  3. Köfteci Ramiz (Kızılay) - Score: 0.5086\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analyzing query: 'Balık restoranı tavsiye eder misin?'\n",
            "==================================================\n",
            "Intent: CUISINE_QUERY (confidence: 0.8872)\n",
            "Processed tokens: ['balık', 'restoranı', 'tavsiye', 'eder', 'misin']\n",
            "Bigrams: ['balık restoranı', 'restoranı tavsiye', 'tavsiye eder', 'eder misin']\n",
            "Extracted entities: {'LOCATION': ['restoranı'], 'CUISINE': ['balık'], 'PRICE': [], 'RATING': ['tavsiye'], 'FEATURE': []}\n",
            "Top 3 matching restaurants:\n",
            "  1. Balık Restaurant (Bahçelievler) - Score: 0.7721\n",
            "  2. Köfteci Ramiz (Kızılay) - Score: 0.3667\n",
            "  3. Burger King (Tunalı) - Score: 0.3594\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analyzing query: 'En iyi burger nerede?'\n",
            "==================================================\n",
            "Intent: CUISINE_QUERY (confidence: 0.8820)\n",
            "Processed tokens: ['iyi', 'burger', 'nerede']\n",
            "Bigrams: ['iyi burger', 'burger nerede']\n",
            "Extracted entities: {'LOCATION': [], 'CUISINE': ['burger'], 'PRICE': [], 'RATING': ['iyi'], 'FEATURE': []}\n",
            "Top 3 matching restaurants:\n",
            "  1. Burger King (Tunalı) - Score: 0.6338\n",
            "  2. Köfteci Ramiz (Kızılay) - Score: 0.4183\n",
            "  3. Balık Restaurant (Bahçelievler) - Score: 0.3138\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize components\n",
        "text_processor = TextProcessor()\n",
        "ner_processor = RestaurantNER()\n",
        "similarity_engine = SimilarityEngine()\n",
        "\n",
        "# Create and process restaurant data\n",
        "restaurant_data = similarity_engine.create_restaurant_data()\n",
        "similarity_engine.create_restaurant_embeddings(restaurant_data)\n",
        "\n",
        "# Example usage\n",
        "def complete_query_analysis(query: str):\n",
        "    \"\"\"\n",
        "    Complete analysis of a restaurant query\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing query: '{query}'\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 1. Intent Recognition (using your existing function)\n",
        "    intent_result = predict_intent(query)\n",
        "    print(f\"Intent: {intent_result['intent']} (confidence: {intent_result['confidence']:.4f})\")\n",
        "    \n",
        "    # 2. Text Processing\n",
        "    processed = text_processor.process_text(query)\n",
        "    print(f\"Processed tokens: {processed['tokens']}\")\n",
        "    print(f\"Bigrams: {processed['bigrams']}\")\n",
        "    \n",
        "    # 3. Named Entity Recognition\n",
        "    entities = ner_processor.extract_entities(query)\n",
        "    print(f\"Extracted entities: {entities}\")\n",
        "    \n",
        "    # 4. Semantic Search\n",
        "    search_results = similarity_engine.semantic_search(query, top_k=3)\n",
        "    print(f\"Top {len(search_results['results'])} matching restaurants:\")\n",
        "    for i, restaurant in enumerate(search_results['results'], 1):\n",
        "        print(f\"  {i}. {restaurant['name']} ({restaurant['location']}) - Score: {restaurant['similarity_score']:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'intent': intent_result,\n",
        "        'processed_text': processed,\n",
        "        'entities': entities,\n",
        "        'search_results': search_results\n",
        "    }\n",
        "\n",
        "# Test examples\n",
        "test_queries = [\n",
        "    \"Kızılay'da iyi köfte nerede yenir?\",\n",
        "    \"Çankaya'da pahalı olmayan pizza yeri\",\n",
        "    \"Balık restoranı tavsiye eder misin?\",\n",
        "    \"En iyi burger nerede?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    result = complete_query_analysis(query)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
