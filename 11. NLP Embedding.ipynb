{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's examine the CSV data and normalize Turkish characters\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_turkish(text):\n",
    "    \"\"\"\n",
    "    Normalize Turkish characters to their ASCII equivalents for comparison.\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # Turkish character mappings\n",
    "    turkish_chars = {\n",
    "        'ç': 'c', 'Ç': 'C',\n",
    "        'ğ': 'g', 'Ğ': 'G', \n",
    "        'ı': 'i', 'I': 'I',\n",
    "        'İ': 'I', 'i': 'i',\n",
    "        'ö': 'o', 'Ö': 'O',\n",
    "        'ş': 's', 'Ş': 'S',\n",
    "        'ü': 'u', 'Ü': 'U'\n",
    "    }\n",
    "    \n",
    "    normalized = text.lower()\n",
    "    for turkish, ascii_char in turkish_chars.items():\n",
    "        normalized = normalized.replace(turkish.lower(), ascii_char.lower())\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Final_Data.csv')\n",
    "\n",
    "# Normalize and get unique cities and districts\n",
    "df['Il_normalized'] = df['Il'].apply(normalize_turkish)\n",
    "df['Ilce_normalized'] = df['Ilce'].apply(normalize_turkish)\n",
    "\n",
    "unique_cities = df['Il_normalized'].dropna().unique()\n",
    "unique_districts = df['Ilce_normalized'].dropna().unique()\n",
    "\n",
    "print(f\"Number of unique cities: {len(unique_cities)}\")\n",
    "print(f\"Cities: {sorted(unique_cities)}\")\n",
    "print(f\"\\nNumber of unique districts: {len(unique_districts)}\")\n",
    "print(f\"First 10 districts: {sorted(unique_districts)[:10]}\")\n",
    "\n",
    "# Show data structure\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc96231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Configuration for LM Studio\n",
    "LM_STUDIO_BASE_URL = \"http://localhost:1234\"\n",
    "MODEL_NAME = \"turkish-gemma-9b-v0.1-i1\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def query_local_llm(prompt, system_prompt, temperature=0):\n",
    "    \"\"\"\n",
    "    Query the local LLM via LM Studio API (OpenAI-compatible).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{LM_STUDIO_BASE_URL}/v1/chat/completions\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": 50,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            print(f\"LLM API error: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LLM connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_location(text, allowed_values, location_type=\"location\"):\n",
    "    \"\"\"\n",
    "    Normalize Turkish location name using LLM + fuzzy fallback.\n",
    "    Skips LLM correction for valid provinces and standalone districts.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return None\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check if already valid (exact match)\n",
    "    normalized_input = normalize_turkish(text)\n",
    "    normalized_allowed = {normalize_turkish(val): val for val in allowed_values}\n",
    "    \n",
    "    if normalized_input in normalized_allowed:\n",
    "        return normalized_allowed[normalized_input]\n",
    "    \n",
    "    # Skip LLM correction in specific cases:\n",
    "    # 1. For provinces (cities) - if it's a recognized province, don't try to correct\n",
    "    # 2. For districts entered without province context\n",
    "    skip_llm = False\n",
    "    \n",
    "    # Check if input is a known Turkish province (even if not in current allowed_values)\n",
    "    turkish_provinces = [\n",
    "        \"adana\", \"adiyaman\", \"afyonkarahisar\", \"agri\", \"aksaray\", \"amasya\", \"ankara\", \"antalya\", \n",
    "        \"ardahan\", \"artvin\", \"aydin\", \"balikesir\", \"bartin\", \"batman\", \"bayburt\", \"bilecik\", \n",
    "        \"bingol\", \"bitlis\", \"bolu\", \"burdur\", \"bursa\", \"canakkale\", \"cankiri\", \"corum\", \n",
    "        \"denizli\", \"diyarbakir\", \"duzce\", \"edirne\", \"elazig\", \"erzincan\", \"erzurum\", \"eskisehir\", \n",
    "        \"gaziantep\", \"giresun\", \"gumushane\", \"hakkari\", \"hatay\", \"igdir\", \"isparta\", \"istanbul\", \n",
    "        \"izmir\", \"izmit\", \"kahramanmaras\", \"karabuk\", \"karaman\", \"kars\", \"kastamonu\", \"kayseri\", \"kilis\", \n",
    "        \"kirikkale\", \"kirklareli\", \"kirsehir\", \"kocaeli\", \"konya\", \"kutahya\", \"malatya\", \"manisa\", \n",
    "        \"mardin\", \"mersin\", \"mugla\", \"mus\", \"nevsehir\", \"nigde\", \"ordu\", \"osmaniye\", \"rize\", \n",
    "        \"sakarya\", \"samsun\", \"sanliurfa\", \"siirt\", \"sinop\", \"sivas\", \"sirnak\", \"tekirdag\", \n",
    "        \"tokat\", \"trabzon\", \"tunceli\", \"usak\", \"van\", \"yalova\", \"yozgat\", \"zonguldak\"\n",
    "    ]\n",
    "    \n",
    "    if location_type == \"city\" and normalize_turkish(text) in turkish_provinces:\n",
    "        # If it's a valid Turkish province, check if it exists in our dataset\n",
    "        if normalized_input in normalized_allowed:\n",
    "            print(f\"Valid Turkish province '{text}' found in dataset\")\n",
    "            return normalized_allowed[normalized_input]\n",
    "        else:\n",
    "            print(f\"Valid Turkish province '{text}' but not available in our dataset\")\n",
    "            return None\n",
    "    elif location_type == \"district\":\n",
    "        # For districts entered standalone, check if it exists in dataset first\n",
    "        if normalized_input in normalized_allowed:\n",
    "            print(f\"District '{text}' found in dataset\")\n",
    "            return normalized_allowed[normalized_input]\n",
    "        else:\n",
    "            # Skip LLM correction for districts, only use fuzzy matching\n",
    "            skip_llm = True\n",
    "            print(f\"District '{text}' not found - trying fuzzy matching only\")\n",
    "    \n",
    "    # Try LLM correction only if not skipping\n",
    "    if not skip_llm:\n",
    "        system_prompt = (\"You are a Turkish location normalizer. Return only the corrected city or district name in Turkish, \"\n",
    "                        \"matching diacritics (İ/ı, Ş, Ğ, Ü, Ö, Ç), with correct title casing. \"\n",
    "                        \"Only correct obvious spelling errors, not valid location names. \"\n",
    "                        \"If unsure, return the closest valid option from the provided list. \"\n",
    "                        \"Output only the name, nothing else.\")\n",
    "        \n",
    "        allowed_list = \", \".join(sorted(allowed_values))\n",
    "        prompt = f\"Correct this Turkish {location_type} name: '{text}'\\nValid options: {allowed_list}\"\n",
    "        \n",
    "        llm_result = query_local_llm(prompt, system_prompt)\n",
    "        \n",
    "        if llm_result:\n",
    "            # Check if LLM result is in allowed values\n",
    "            llm_normalized = normalize_turkish(llm_result)\n",
    "            if llm_normalized in normalized_allowed:\n",
    "                corrected = normalized_allowed[llm_normalized]\n",
    "                if corrected != text:\n",
    "                    print(f\"LLM corrected '{text}' → '{corrected}'\")\n",
    "                return corrected\n",
    "    \n",
    "    # Fallback to fuzzy matching\n",
    "    fuzzy_matches = get_close_matches(text.lower(), \n",
    "                                    [val.lower() for val in allowed_values], \n",
    "                                    n=1, cutoff=0.6)\n",
    "    \n",
    "    if fuzzy_matches:\n",
    "        # Find original case version\n",
    "        for val in allowed_values:\n",
    "            if val.lower() == fuzzy_matches[0]:\n",
    "                print(f\"Fuzzy matched '{text}' → '{val}'\")\n",
    "                return val\n",
    "    \n",
    "    return None  # No correction found\n",
    "\n",
    "print(\"LM Studio client and location normalizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_and_filter_restaurants_enhanced():\n",
    "    \"\"\"\n",
    "    Enhanced function with auto-correction for Turkish location names.\n",
    "    1. Gets location input from user\n",
    "    2. Auto-corrects using local LLM + fuzzy matching\n",
    "    3. Filters restaurants based on corrected input\n",
    "    4. Returns filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Load data and normalize Turkish characters\n",
    "    df = pd.read_csv('Final_Data.csv')\n",
    "    df['Il_normalized'] = df['Il'].apply(normalize_turkish)\n",
    "    df['Ilce_normalized'] = df['Ilce'].apply(normalize_turkish)\n",
    "    \n",
    "    available_cities = list(df['Il'].dropna().unique())\n",
    "    available_districts = list(df['Ilce'].dropna().unique())\n",
    "    \n",
    "    print(\"Restaurant Recommendation System (with Auto-Correction)\")\n",
    "    print(\"Please provide location information:\")\n",
    "    \n",
    "    # Get user input with auto-correction\n",
    "    while True:\n",
    "        city_input = input(\"Enter city (or press Enter to skip): \").strip()\n",
    "        district_input = input(\"Enter district (or press Enter to skip): \").strip()\n",
    "        \n",
    "        # Check if at least one is provided\n",
    "        if not city_input and not district_input:\n",
    "            print(\"Warning: At least one location (city or district) must be provided!\")\n",
    "            continue\n",
    "        \n",
    "        # Auto-correct city if provided\n",
    "        city_corrected = None\n",
    "        if city_input:\n",
    "            city_corrected = normalize_location(city_input, available_cities, \"city\")\n",
    "            if not city_corrected:\n",
    "                print(f\"City '{city_input}' is not available in our dataset. Please try again.\")\n",
    "                print(f\"Available cities in dataset: {sorted(available_cities)}\")\n",
    "                continue\n",
    "        \n",
    "        # Auto-correct district if provided\n",
    "        district_corrected = None\n",
    "        if district_input:\n",
    "            district_corrected = normalize_location(district_input, available_districts, \"district\")\n",
    "            if not district_corrected:\n",
    "                print(f\"District '{district_input}' is not available in our dataset. Please try again.\")\n",
    "                print(f\"First 10 available districts in dataset: {sorted(available_districts)[:10]}...\")\n",
    "                continue\n",
    "        \n",
    "        # If we get here, both inputs are valid\n",
    "        break\n",
    "    \n",
    "    print(f\"Final location: City='{city_corrected}', District='{district_corrected}'\")\n",
    "    \n",
    "    # Filter restaurants based on corrected input\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Apply city filter if provided\n",
    "    if city_corrected:\n",
    "        filtered_df = filtered_df[filtered_df['Il'] == city_corrected]\n",
    "        print(f\"Filtered by city '{city_corrected}': {len(filtered_df)} restaurants found\")\n",
    "    \n",
    "    # Apply district filter if provided\n",
    "    if district_corrected:\n",
    "        filtered_df = filtered_df[filtered_df['Ilce'] == district_corrected]\n",
    "        print(f\"Filtered by district '{district_corrected}': {len(filtered_df)} restaurants found\")\n",
    "    \n",
    "    print(f\"Final filtered results: {len(filtered_df)} restaurants\")\n",
    "    \n",
    "    # Show sample results\n",
    "    if len(filtered_df) > 0:\n",
    "        print(f\"\\nSample restaurants:\")\n",
    "        print(filtered_df[['Mekan_Adı', 'Il', 'Ilce', 'Total_Weighted_Score']].head())\n",
    "    else:\n",
    "        print(\"No restaurants found for the given location.\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Test the enhanced function\n",
    "# Start LM Studio first, then uncomment below:\n",
    "filtered_restaurants = get_location_and_filter_restaurants_enhanced()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc205fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from zemberek import TurkishSentenceNormalizer, TurkishMorphology\n",
    "\n",
    "df = filtered_restaurants\n",
    "morph = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morph)\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# embed all summaries\n",
    "summ_texts = df['Özet'].fillna(\"\").astype(str).tolist()\n",
    "summ_emb = sbert.encode(summ_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# embed all keywords\n",
    "keywords_texts = df['Keywords'].fillna(\"\").astype(str).tolist()\n",
    "keywords_emb = sbert.encode(keywords_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# query\n",
    "q = input(\"Cümleni yaz: \")\n",
    "q_norm = normalizer.normalize(q)\n",
    "q_emb = sbert.encode(q_norm, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# similarity\n",
    "sims_summ = summ_emb @ q_emb\n",
    "sims_kw = keywords_emb @ q_emb\n",
    "\n",
    "# weighted similarity (80% summaries + 20% keywords)\n",
    "sims = (0.8 * sims_summ) + (0.2 * sims_kw)\n",
    "\n",
    "# top-5 by weighted similarity\n",
    "top5_idx = np.argsort(-sims)[:5]\n",
    "top5_df = df.iloc[top5_idx].copy()\n",
    "top5_df[\"sim_score\"] = sims[top5_idx]\n",
    "\n",
    "# sort again by Total_Weighted_Score (desc)\n",
    "top5_sorted = top5_df.sort_values(by=\"Total_Weighted_Score\", ascending=False)\n",
    "\n",
    "# print results\n",
    "for _, row in top5_sorted.iterrows():\n",
    "    print(f\"{row['Mekan_Adı']}  (sim={row['sim_score']:.3f}, total={row['Total_Weighted_Score']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
