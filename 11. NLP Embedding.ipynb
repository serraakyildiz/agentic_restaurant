{"cells":[{"cell_type":"markdown","source":["This notebook implements a natural language processing system for restaurant recommendation using semantic embeddings and similarity search. The system processes Turkish restaurant data through character normalization, location input correction, semantic embedding generation, and hybrid similarity matching. Built on Sentence-BERT architecture with the all-MiniLM-L6-v2 model, it generates dense vector representations of restaurant summaries and keywords to enable semantic understanding of user queries. The pipeline incorporates Turkish language processing through Zemberek morphology library for proper text normalization and character handling. Location input correction utilizes a hybrid approach combining local language model inference through LM Studio API with fuzzy string matching to automatically correct misspelled city and district names. The semantic search system implements weighted similarity scoring that combines restaurant summary embeddings with keyword embeddings using an 80-20 weighting scheme. Query processing involves normalization of user input, embedding generation using Sentence-BERT, and cosine similarity computation against pre-computed restaurant embeddings. The final recommendation ranking combines semantic similarity scores with restaurant quality metrics through Total Weighted Score, ensuring recommendations are both semantically relevant and of high quality. The system supports location-based filtering with automatic correction capabilities and includes comprehensive Turkish character normalization and format standardization for consistent embedding generation."],"metadata":{"id":"wxSqPMNSRUht"},"id":"wxSqPMNSRUht"},{"cell_type":"code","execution_count":null,"id":"23031e79","metadata":{"id":"23031e79","outputId":"2e21c333-2318-4128-e9f0-f92618a88c5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: zemberek-python in /opt/anaconda3/lib/python3.12/site-packages (0.2.3)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /opt/anaconda3/lib/python3.12/site-packages (from zemberek-python) (4.8)\n","Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from zemberek-python) (1.26.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install zemberek-python"]},{"cell_type":"code","execution_count":null,"id":"88c7a481","metadata":{"id":"88c7a481","outputId":"16286965-c498-4987-dac1-f331477c74f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique cities: 3\n","Cities: ['ankara', 'istanbul', 'izmir']\n","\n","Number of unique districts: 75\n","First 10 districts: ['adalar', 'akyurt', 'alsancak', 'altindag', 'arnavutkoy', 'atasehir', 'avcilar', 'ayas', 'bagcilar', 'bahcelievler']\n","\n","Dataset shape: (2365, 13)\n","Columns: ['Mekan_Adı', 'Ilce', 'Il', 'Total_Weighted_Score', 'Özet', 'Fiyat-Performans', 'Hizmet', 'Menü Çeşitliliği', 'Ortam', 'Tat', 'Temizlik', 'Il_normalized', 'Ilce_normalized']\n"]}],"source":["# First, let's examine the CSV data and normalize Turkish characters\n","import pandas as pd\n","\n","def normalize_turkish(text):\n","    \"\"\"\n","    Normalize Turkish characters to their ASCII equivalents for comparison.\n","    \"\"\"\n","    if not text or pd.isna(text):\n","        return text\n","\n","    # Turkish character mappings\n","    turkish_chars = {\n","        'ç': 'c', 'Ç': 'C',\n","        'ğ': 'g', 'Ğ': 'G',\n","        'ı': 'i', 'I': 'I',\n","        'İ': 'I', 'i': 'i',\n","        'ö': 'o', 'Ö': 'O',\n","        'ş': 's', 'Ş': 'S',\n","        'ü': 'u', 'Ü': 'U'\n","    }\n","\n","    normalized = text.lower()\n","    for turkish, ascii_char in turkish_chars.items():\n","        normalized = normalized.replace(turkish.lower(), ascii_char.lower())\n","\n","    return normalized\n","\n","# Load the data\n","df = pd.read_csv('/Users/Serra/Desktop/bitirme/kullanılan csvler/Final_Data.csv')\n","\n","# Normalize and get unique cities and districts\n","df['Il_normalized'] = df['Il'].apply(normalize_turkish)\n","df['Ilce_normalized'] = df['Ilce'].apply(normalize_turkish)\n","\n","unique_cities = df['Il_normalized'].dropna().unique()\n","unique_districts = df['Ilce_normalized'].dropna().unique()\n","\n","print(f\"Number of unique cities: {len(unique_cities)}\")\n","print(f\"Cities: {sorted(unique_cities)}\")\n","print(f\"\\nNumber of unique districts: {len(unique_districts)}\")\n","print(f\"First 10 districts: {sorted(unique_districts)[:10]}\")\n","\n","# Show data structure\n","print(f\"\\nDataset shape: {df.shape}\")\n","print(f\"Columns: {df.columns.tolist()}\")"]},{"cell_type":"code","execution_count":null,"id":"5fc96231","metadata":{"id":"5fc96231","outputId":"9cba569a-fd98-4bd0-92ce-e6cfa1b4473d"},"outputs":[{"name":"stdout","output_type":"stream","text":["LM Studio client and location normalizer ready!\n"]}],"source":["import pandas as pd\n","\n","# Configuration for LM Studio\n","LM_STUDIO_BASE_URL = \"http://localhost:1234\"\n","MODEL_NAME = \"turkish-gemma-9b-v0.1-i1\"\n","\n","import requests\n","import json\n","from difflib import get_close_matches\n","\n","def query_local_llm(prompt, system_prompt, temperature=0):\n","    \"\"\"\n","    Query the local LLM via LM Studio API (OpenAI-compatible).\n","    \"\"\"\n","    try:\n","        response = requests.post(\n","            f\"{LM_STUDIO_BASE_URL}/v1/chat/completions\",\n","            headers={\"Content-Type\": \"application/json\"},\n","            json={\n","                \"model\": MODEL_NAME,\n","                \"messages\": [\n","                    {\"role\": \"system\", \"content\": system_prompt},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                \"temperature\": temperature,\n","                \"max_tokens\": 50,\n","                \"stream\": False\n","            },\n","            timeout=10\n","        )\n","\n","        if response.status_code == 200:\n","            result = response.json()\n","            return result[\"choices\"][0][\"message\"][\"content\"].strip()\n","        else:\n","            print(f\"LLM API error: {response.status_code}\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"LLM connection failed: {e}\")\n","        return None\n","\n","def normalize_location(text, allowed_values, location_type=\"location\"):\n","    \"\"\"\n","    Normalize Turkish location name using LLM + fuzzy fallback.\n","    Skips LLM correction for valid provinces and standalone districts.\n","    \"\"\"\n","    if not text or not text.strip():\n","        return None\n","\n","    text = text.strip()\n","\n","    # Check if already valid (exact match)\n","    normalized_input = normalize_turkish(text)\n","    normalized_allowed = {normalize_turkish(val): val for val in allowed_values}\n","\n","    if normalized_input in normalized_allowed:\n","        return normalized_allowed[normalized_input]\n","\n","    # Skip LLM correction in specific cases:\n","    # 1. For provinces (cities) - if it's a recognized province, don't try to correct\n","    # 2. For districts entered without province context\n","    skip_llm = False\n","\n","    # Check if input is a known Turkish province (even if not in current allowed_values)\n","    turkish_provinces = [\n","        \"adana\", \"adiyaman\", \"afyonkarahisar\", \"agri\", \"aksaray\", \"amasya\", \"ankara\", \"antalya\",\n","        \"ardahan\", \"artvin\", \"aydin\", \"balikesir\", \"bartin\", \"batman\", \"bayburt\", \"bilecik\",\n","        \"bingol\", \"bitlis\", \"bolu\", \"burdur\", \"bursa\", \"canakkale\", \"cankiri\", \"corum\",\n","        \"denizli\", \"diyarbakir\", \"duzce\", \"edirne\", \"elazig\", \"erzincan\", \"erzurum\", \"eskisehir\",\n","        \"gaziantep\", \"giresun\", \"gumushane\", \"hakkari\", \"hatay\", \"igdir\", \"isparta\", \"istanbul\",\n","        \"izmir\", \"izmit\", \"kahramanmaras\", \"karabuk\", \"karaman\", \"kars\", \"kastamonu\", \"kayseri\", \"kilis\",\n","        \"kirikkale\", \"kirklareli\", \"kirsehir\", \"kocaeli\", \"konya\", \"kutahya\", \"malatya\", \"manisa\",\n","        \"mardin\", \"mersin\", \"mugla\", \"mus\", \"nevsehir\", \"nigde\", \"ordu\", \"osmaniye\", \"rize\",\n","        \"sakarya\", \"samsun\", \"sanliurfa\", \"siirt\", \"sinop\", \"sivas\", \"sirnak\", \"tekirdag\",\n","        \"tokat\", \"trabzon\", \"tunceli\", \"usak\", \"van\", \"yalova\", \"yozgat\", \"zonguldak\"\n","    ]\n","\n","    if location_type == \"city\" and normalize_turkish(text) in turkish_provinces:\n","        # If it's a valid Turkish province, check if it exists in our dataset\n","        if normalized_input in normalized_allowed:\n","            print(f\"Valid Turkish province '{text}' found in dataset\")\n","            return normalized_allowed[normalized_input]\n","        else:\n","            print(f\"Valid Turkish province '{text}' but not available in our dataset\")\n","            return None\n","    elif location_type == \"district\":\n","        # For districts entered standalone, check if it exists in dataset first\n","        if normalized_input in normalized_allowed:\n","            print(f\"District '{text}' found in dataset\")\n","            return normalized_allowed[normalized_input]\n","        else:\n","            # Skip LLM correction for districts, only use fuzzy matching\n","            skip_llm = True\n","            print(f\"District '{text}' not found - trying fuzzy matching only\")\n","\n","    # Try LLM correction only if not skipping\n","    if not skip_llm:\n","        system_prompt = (\"You are a Turkish location normalizer. Return only the corrected city or district name in Turkish, \"\n","                        \"matching diacritics (İ/ı, Ş, Ğ, Ü, Ö, Ç), with correct title casing. \"\n","                        \"Only correct obvious spelling errors, not valid location names. \"\n","                        \"If unsure, return the closest valid option from the provided list. \"\n","                        \"Output only the name, nothing else.\")\n","\n","        allowed_list = \", \".join(sorted(allowed_values))\n","        prompt = f\"Correct this Turkish {location_type} name: '{text}'\\nValid options: {allowed_list}\"\n","\n","        llm_result = query_local_llm(prompt, system_prompt)\n","\n","        if llm_result:\n","            # Check if LLM result is in allowed values\n","            llm_normalized = normalize_turkish(llm_result)\n","            if llm_normalized in normalized_allowed:\n","                corrected = normalized_allowed[llm_normalized]\n","                if corrected != text:\n","                    print(f\"LLM corrected '{text}' → '{corrected}'\")\n","                return corrected\n","\n","    # Fallback to fuzzy matching\n","    fuzzy_matches = get_close_matches(text.lower(),\n","                                    [val.lower() for val in allowed_values],\n","                                    n=1, cutoff=0.6)\n","\n","    if fuzzy_matches:\n","        # Find original case version\n","        for val in allowed_values:\n","            if val.lower() == fuzzy_matches[0]:\n","                print(f\"Fuzzy matched '{text}' → '{val}'\")\n","                return val\n","\n","    return None  # No correction found\n","\n","print(\"LM Studio client and location normalizer ready!\")"]},{"cell_type":"code","execution_count":null,"id":"a527c15a","metadata":{"id":"a527c15a","outputId":"c3d26e71-5e0a-4573-d12d-1dc0032411e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Restaurant Recommendation System (with Auto-Correction)\n","Please provide location information:\n","Final location: City='İstanbul', District='Kadıköy'\n","Filtered by city 'İstanbul': 1003 restaurants found\n","Filtered by district 'Kadıköy': 39 restaurants found\n","Final filtered results: 39 restaurants\n","\n","Sample restaurants:\n","                           Mekan_Adı        Il     Ilce  Total_Weighted_Score\n","28                     5masa_kalamis  İstanbul  Kadıköy              0.149464\n","29                          700_gram  İstanbul  Kadıköy              0.226943\n","63                    affan_ocakbasi  İstanbul  Kadıköy              0.296856\n","106                    alef_ocakbasi  İstanbul  Kadıköy              0.198858\n","179  antebi_restaurant_ciftehavuzlar  İstanbul  Kadıköy              0.137211\n"]}],"source":["def get_location_and_filter_restaurants_enhanced():\n","    \"\"\"\n","    Enhanced function with auto-correction for Turkish location names.\n","    1. Gets location input from user\n","    2. Auto-corrects using local LLM + fuzzy matching\n","    3. Filters restaurants based on corrected input\n","    4. Returns filtered DataFrame\n","    \"\"\"\n","    # Load data and normalize Turkish characters\n","    df = pd.read_csv('/Users/Serra/Desktop/bitirme/kullanılan csvler/Final_Data.csv')\n","    df['Il_normalized'] = df['Il'].apply(normalize_turkish)\n","    df['Ilce_normalized'] = df['Ilce'].apply(normalize_turkish)\n","\n","    available_cities = list(df['Il'].dropna().unique())\n","    available_districts = list(df['Ilce'].dropna().unique())\n","\n","    print(\"Restaurant Recommendation System (with Auto-Correction)\")\n","    print(\"Please provide location information:\")\n","\n","    # Get user input with auto-correction\n","    while True:\n","        city_input = input(\"Enter city (or press Enter to skip): \").strip()\n","        district_input = input(\"Enter district (or press Enter to skip): \").strip()\n","\n","        # Check if at least one is provided\n","        if not city_input and not district_input:\n","            print(\"Warning: At least one location (city or district) must be provided!\")\n","            continue\n","\n","        # Auto-correct city if provided\n","        city_corrected = None\n","        if city_input:\n","            city_corrected = normalize_location(city_input, available_cities, \"city\")\n","            if not city_corrected:\n","                print(f\"City '{city_input}' is not available in our dataset. Please try again.\")\n","                print(f\"Available cities in dataset: {sorted(available_cities)}\")\n","                continue\n","\n","        # Auto-correct district if provided\n","        district_corrected = None\n","        if district_input:\n","            district_corrected = normalize_location(district_input, available_districts, \"district\")\n","            if not district_corrected:\n","                print(f\"District '{district_input}' is not available in our dataset. Please try again.\")\n","                print(f\"First 10 available districts in dataset: {sorted(available_districts)[:10]}...\")\n","                continue\n","\n","        # If we get here, both inputs are valid\n","        break\n","\n","    print(f\"Final location: City='{city_corrected}', District='{district_corrected}'\")\n","\n","    # Filter restaurants based on corrected input\n","    filtered_df = df.copy()\n","\n","    # Apply city filter if provided\n","    if city_corrected:\n","        filtered_df = filtered_df[filtered_df['Il'] == city_corrected]\n","        print(f\"Filtered by city '{city_corrected}': {len(filtered_df)} restaurants found\")\n","\n","    # Apply district filter if provided\n","    if district_corrected:\n","        filtered_df = filtered_df[filtered_df['Ilce'] == district_corrected]\n","        print(f\"Filtered by district '{district_corrected}': {len(filtered_df)} restaurants found\")\n","\n","    print(f\"Final filtered results: {len(filtered_df)} restaurants\")\n","\n","    # Show sample results\n","    if len(filtered_df) > 0:\n","        print(f\"\\nSample restaurants:\")\n","        print(filtered_df[['Mekan_Adı', 'Il', 'Ilce', 'Total_Weighted_Score']].head())\n","    else:\n","        print(\"No restaurants found for the given location.\")\n","\n","    return filtered_df\n","\n","# Test the enhanced function\n","# Start LM Studio first, then uncomment below:\n","filtered_restaurants = get_location_and_filter_restaurants_enhanced()\n"]},{"cell_type":"code","execution_count":null,"id":"cc205fa5","metadata":{"colab":{"referenced_widgets":["ad25e90f31c84872a257088f3544579e"]},"id":"cc205fa5","outputId":"ac8af991-f7f1-42be-f05a-dc0719a4fbe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-09-03 21:27:48,839 - zemberek.morphology.turkish_morphology - INFO\n","Msg: TurkishMorphology instance initialized in 1.9773387908935547\n","\n","2025-09-03 21:27:52,074 - sentence_transformers.SentenceTransformer - INFO\n","Msg: Use pytorch device_name: mps\n","\n","2025-09-03 21:27:52,074 - sentence_transformers.SentenceTransformer - INFO\n","Msg: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad25e90f31c84872a257088f3544579e","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyError","evalue":"'Keywords'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Keywords'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m summ_emb \u001b[38;5;241m=\u001b[39m sbert\u001b[38;5;241m.\u001b[39mencode(summ_texts, convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# embed all keywords\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m keywords_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeywords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     17\u001b[0m keywords_emb \u001b[38;5;241m=\u001b[39m sbert\u001b[38;5;241m.\u001b[39mencode(keywords_texts, convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# query\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'Keywords'"]}],"source":["import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","import numpy as np\n","from zemberek import TurkishSentenceNormalizer, TurkishMorphology\n","\n","df = filtered_restaurants\n","morph = TurkishMorphology.create_with_defaults()\n","normalizer = TurkishSentenceNormalizer(morph)\n","sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# embed all summaries\n","summ_texts = df['Özet'].fillna(\"\").astype(str).tolist()\n","summ_emb = sbert.encode(summ_texts, convert_to_numpy=True, normalize_embeddings=True)\n","\n","# embed all keywords\n","keywords_texts = df['Keywords'].fillna(\"\").astype(str).tolist()\n","keywords_emb = sbert.encode(keywords_texts, convert_to_numpy=True, normalize_embeddings=True)\n","\n","# query\n","q = input(\"Cümleni yaz: \")\n","q_norm = normalizer.normalize(q)\n","q_emb = sbert.encode(q_norm, convert_to_numpy=True, normalize_embeddings=True)\n","\n","# similarity\n","sims_summ = summ_emb @ q_emb\n","sims_kw = keywords_emb @ q_emb\n","\n","# weighted similarity (80% summaries + 20% keywords)\n","sims = (0.8 * sims_summ) + (0.2 * sims_kw)\n","\n","# top-5 by weighted similarity\n","top5_idx = np.argsort(-sims)[:5]\n","top5_df = df.iloc[top5_idx].copy()\n","top5_df[\"sim_score\"] = sims[top5_idx]\n","\n","# sort again by Total_Weighted_Score (desc)\n","top5_sorted = top5_df.sort_values(by=\"Total_Weighted_Score\", ascending=False)\n","\n","# print results\n","for _, row in top5_sorted.iterrows():\n","    print(f\"{row['Mekan_Adı']}  (sim={row['sim_score']:.3f}, total={row['Total_Weighted_Score']})\")"]},{"cell_type":"code","execution_count":null,"id":"3141422a","metadata":{"id":"3141422a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}